{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.3 64-bit ('base': conda)",
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "14737f34d33bafd070bec9101b483950d2d3e55091a34c66760d56e0a728b202"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import useful packages\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define useful functions\n",
    "def create_output_file(y_actual, y_predicted, labels, filename):\n",
    "    '''\n",
    "    Function for creating the ouput file as specified in the assignment instructions:\n",
    "    a) instance number and predicted label (number)\n",
    "    b) Confusion matrix\n",
    "    c) Precision, recall and f1-measure of each class\n",
    "    d) Accuracy, macro-average f1 and weighted-average f1 score of the model\n",
    "\n",
    "    y_actual: numpy array of shape (N,) containing the actual class of each test instance\n",
    "    y_predicted: numpy array of shape (N,) containing the class of each test instance predicted by the model\n",
    "    labels: 1D numpy array containing the class labels of the dataset\n",
    "    filename: name of the output (.csv) file\n",
    "    '''\n",
    "\n",
    "    # Open file\n",
    "    output = open('A1-Output/' + filename + '.csv', 'w')\n",
    "\n",
    "    # a) Write y values of test data\n",
    "    output.write('instance,prediction\\n')\n",
    "    for i in range(y_predicted.shape[0]):\n",
    "        output.write(str(i+1) + ',' + str(y_predicted[i]) + '\\n')\n",
    "\n",
    "    output.write('\\n')\n",
    "\n",
    "    # b) Plot confusion matrix\n",
    "    output.write('confusion matrix\\n')\n",
    "    confusion_matrix = sklearn.metrics.confusion_matrix(y_actual, y_predicted)\n",
    "    (m, n) = confusion_matrix.shape\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            if j < n-1:\n",
    "                output.write(str(confusion_matrix[i,j]) + ',')\n",
    "            else:\n",
    "                output.write(str(confusion_matrix[i,j]))\n",
    "        output.write('\\n')\n",
    "\n",
    "    output.write('\\n')\n",
    "\n",
    "    # c) Write precision, recall and f1-measure of each class (rounded to 2 decimals)\n",
    "    output.write('precision,recall,f1-measure\\n')\n",
    "    precision = sklearn.metrics.precision_score(y_actual, y_predicted, average=None)\n",
    "    recall = sklearn.metrics.recall_score(y_actual, y_predicted, average=None)\n",
    "    f1 = sklearn.metrics.f1_score(y_actual, y_predicted, average=None)\n",
    "\n",
    "    for i in range(labels.shape[0]):\n",
    "        #output.write(str(precision[i]) + ',' + str(recall[i]) + ',' + str(f1[i]) + '\\n')\n",
    "        output.write('{:.2f},{:.2f},{:.2f}\\n'.format(precision[i], recall[i], f1[i]))\n",
    "\n",
    "    output.write('\\n')\n",
    "\n",
    "    # Write accuracy, macro-average f1 and weighted-average f1 of the model (rounded to 2 decimals)\n",
    "    output.write('accuracy,macro-average f-1,weighted-average f1\\n') \n",
    "    accuracy = sklearn.metrics.accuracy_score(y_actual, y_predicted)\n",
    "    macro_avg_f1 = sklearn.metrics.f1_score(y_actual, y_predicted, average='macro')\n",
    "    weighted_avg_f1 = sklearn.metrics.f1_score(y_actual, y_predicted, average='weighted')\n",
    "    #output.write(str(accuracy) + ',' + str(macro_avg_f1) + ',' + str(weighted_avg_f1))\n",
    "    output.write('{:.2f},{:.2f},{:.2f}\\n'.format(accuracy, macro_avg_f1, weighted_avg_f1))\n",
    "\n",
    "    # Close output file\n",
    "    output.close()\n",
    "\n",
    "def load_dataset(filename, nb_pixels=32**2):\n",
    "    '''\n",
    "    Function for loading the X and Y data of the passed csv file\n",
    "\n",
    "    filename: name of the file containing the dataset (ex: train_1)\n",
    "\n",
    "    Return: \n",
    "    X: 2D numpy array containing the value of the features of each instance\n",
    "    Y: 1D numpy array containing the true class of each instance \n",
    "    '''\n",
    "    data = np.loadtxt('Assig1-Dataset/' + filename + '.csv', delimiter=',', dtype=np.int32)\n",
    "    return data[:, :nb_pixels], data[:, nb_pixels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and validation data for dataset 1\n",
    "ds1_labels = np.loadtxt('Assig1-Dataset/info_1.csv', skiprows=1, usecols=1, delimiter=',', dtype=np.str)\n",
    "\n",
    "ds1_training_X, ds1_training_Y = load_dataset('train_1')\n",
    "ds1_val_X, ds1_val_Y = load_dataset('val_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Perceptron model for dataset 1\n",
    "ds1_perceptron_model = sklearn.linear_model.Perceptron().fit(ds1_training_X, ds1_training_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 7  0  0  0  0  1  0  0  0  0  0  0  0  1  0  0  0  1  0  0  0  0  0  0\n   0  0]\n [ 0  6  0  0  0  1  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0]\n [ 0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0]\n [ 0  1  0  6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0\n   0  0]\n [ 0  1  0  0  4  2  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  5  0  1  0  0  0  0  1  0  0  1  0  0  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  1  0  8  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  6  0  0  0  0  0  0  0  1  0  0  0  0  0  1  1  0\n   0  0]\n [ 1  0  0  0  0  0  0  0  7  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   2  0]\n [ 2  0  0  0  0  1  0  0  0  7  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  0  9  0  0  1  0  0  0  0  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  8  1  0  0  0  0  0  0  0  1  0  0\n   0  0]\n [ 1  0  0  0  0  0  0  0  0  0  0  0  0  6  0  0  0  1  0  0  0  1  0  0\n   1  0]\n [ 0  0  0  0  0  0  1  0  0  1  0  0  0  0  8  0  0  0  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  1  0  0  0  0  0  0  1  0  0  8  0  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0  0  0  0\n   0  0]\n [ 0  1  0  1  0  0  1  0  0  1  0  0  0  0  0  0  0  0  5  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  1  0  0  0  4  0  0  0  0\n   1  0]\n [ 0  0  0  0  0  0  0  0  0  1  0  2  0  0  0  0  0  0  0  0  5  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  4  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  9  0\n   0  0]\n [ 1  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  1  0  0  0  1  0  4\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0\n   7  0]\n [ 0  1  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  7]]\n\n\n              precision    recall  f1-score   support\n\n           A       0.58      0.70      0.64        10\n           B       0.60      0.67      0.63         9\n           C       1.00      1.00      1.00        10\n           D       0.86      0.75      0.80         8\n           E       0.80      0.50      0.62         8\n           F       0.42      0.62      0.50         8\n           G       0.73      0.80      0.76        10\n           H       0.86      0.67      0.75         9\n           I       1.00      0.70      0.82        10\n           J       0.54      0.70      0.61        10\n           K       1.00      0.90      0.95        10\n           L       0.77      1.00      0.87        10\n           M       0.89      0.80      0.84        10\n           N       0.38      0.60      0.46        10\n           O       1.00      0.80      0.89        10\n           P       0.77      1.00      0.87        10\n           Q       1.00      0.80      0.89        10\n           R       0.77      1.00      0.87        10\n           S       1.00      0.56      0.71         9\n           T       1.00      0.50      0.67         8\n           U       0.71      0.62      0.67         8\n           V       0.50      0.50      0.50         8\n           W       0.90      1.00      0.95         9\n           X       1.00      0.50      0.67         8\n           Y       0.64      0.88      0.74         8\n           Z       1.00      0.78      0.88         9\n\n    accuracy                           0.75       239\n   macro avg       0.80      0.74      0.75       239\nweighted avg       0.80      0.75      0.76       239\n\n"
     ]
    }
   ],
   "source": [
    "# Use validation data to test first\n",
    "ds1_val_Y_predict = ds1_perceptron_model.predict(ds1_val_X)\n",
    "\n",
    "# Check validation metrics and modify hyper-parameters as needed in previous cell\n",
    "print (sklearn.metrics.confusion_matrix(ds1_val_Y, ds1_val_Y_predict)) # confusion matrix\n",
    "print ('\\n')\n",
    "print (sklearn.metrics.classification_report(ds1_val_Y, ds1_val_Y_predict, target_names=ds1_labels)) # precision, recall, f1-measure (macro and weighted) and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n [0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 3 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n [1 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2 0 0 0 0 0 0 0]\n [0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n [0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]]\n\n\n              precision    recall  f1-score   support\n\n           A       0.60      0.75      0.67         4\n           B       1.00      1.00      1.00         2\n           C       1.00      0.75      0.86         4\n           D       1.00      1.00      1.00         3\n           E       1.00      1.00      1.00         2\n           F       0.33      0.50      0.40         2\n           G       1.00      0.75      0.86         4\n           H       0.33      0.33      0.33         3\n           I       1.00      1.00      1.00         3\n           J       0.75      0.75      0.75         4\n           K       0.33      0.33      0.33         3\n           L       0.80      1.00      0.89         4\n           M       0.50      0.33      0.40         3\n           N       1.00      1.00      1.00         4\n           O       1.00      1.00      1.00         3\n           P       0.75      1.00      0.86         3\n           Q       0.75      1.00      0.86         3\n           R       0.43      0.75      0.55         4\n           S       1.00      0.67      0.80         3\n           T       0.00      0.00      0.00         2\n           U       1.00      1.00      1.00         3\n           V       1.00      1.00      1.00         3\n           W       1.00      1.00      1.00         3\n           X       1.00      0.50      0.67         2\n           Y       1.00      1.00      1.00         3\n           Z       1.00      0.33      0.50         3\n\n    accuracy                           0.78        80\n   macro avg       0.79      0.76      0.76        80\nweighted avg       0.80      0.78      0.77        80\n\n"
     ]
    }
   ],
   "source": [
    "# When ready, do testing\n",
    "ds1_test_X, ds1_test_Y = load_dataset('test_with_label_1')\n",
    "\n",
    "ds1_test_Y_predict = ds1_perceptron_model.predict(ds1_test_X)\n",
    "\n",
    "# Check test metrics\n",
    "print (sklearn.metrics.confusion_matrix(ds1_test_Y, ds1_test_Y_predict)) # confusion matrix\n",
    "print ('\\n')\n",
    "print (sklearn.metrics.classification_report(ds1_test_Y, ds1_test_Y_predict, target_names=ds1_labels)) # precision, recall, f1-measure (macro and weighted average) and accuracy\n",
    "\n",
    "# Write test results to output file PER-DS1.csv\n",
    "create_output_file(ds1_test_Y, ds1_test_Y_predict, ds1_labels, 'PER-DS1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added: Test with 26.csv file\n",
    "ds1_test_X_26 = np.loadtxt('Assig1-Dataset/26.csv', delimiter=',', dtype=np.int32)\n",
    "ds1_test_Y_predict_26 = ds1_perceptron_model.predict(ds1_test_X_26)\n",
    "\n",
    "# Write to output file\n",
    "output = open('A1-Output/PER-DS1-26.csv', 'w')\n",
    "for i in range(ds1_test_Y_predict_26.shape[0]):\n",
    "    output.write(str(i+1) + ',' + str(ds1_test_Y_predict_26[i]) + '\\n')\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and validation data for dataset 2\n",
    "ds2_labels = np.loadtxt('Assig1-Dataset/info_2.csv', skiprows=1, usecols=1, delimiter=',', dtype=np.str)\n",
    "\n",
    "ds2_training_X, ds2_training_Y = load_dataset('train_2')\n",
    "ds2_val_X, ds2_val_Y = load_dataset('val_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Perceptron model for dataset 2\n",
    "ds2_perceptron_model = sklearn.linear_model.Perceptron().fit(ds2_training_X, ds2_training_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[155   4   0   2   0   0   0   0   3   1]\n [  3 368   0   1   1   0   1   0   1   0]\n [  3   2  28   1   1   1   0   0   8   1]\n [  2   8   0  34   0   1   0   0   0   0]\n [ 30   2   0   0  76   7   1   0   5  29]\n [  2  11   0   3   4 137   1   1   0   6]\n [  1   6   0   0   0   0  37   0   0   1]\n [  3   4   0   0   0   2   0  36   0   0]\n [ 17  10   1   0   1   0   2   0 118   1]\n [ 10   6   2   1   4  11   2   0   5 334]]\n\n\n              precision    recall  f1-score   support\n\n          pi       0.69      0.94      0.79       165\n       alpha       0.87      0.98      0.92       375\n        beta       0.90      0.62      0.74        45\n       sigma       0.81      0.76      0.78        45\n       gamma       0.87      0.51      0.64       150\n       delta       0.86      0.83      0.85       165\n      lambda       0.84      0.82      0.83        45\n       omega       0.97      0.80      0.88        45\n          mu       0.84      0.79      0.81       150\n          xi       0.90      0.89      0.89       375\n\n    accuracy                           0.85      1560\n   macro avg       0.86      0.79      0.81      1560\nweighted avg       0.86      0.85      0.84      1560\n\n"
     ]
    }
   ],
   "source": [
    "# Use validation data to test first\n",
    "ds2_val_Y_predict = ds2_perceptron_model.predict(ds2_val_X)\n",
    "\n",
    "# Check validation metrics and modify hyper-parameters as needed in previous cell\n",
    "print (sklearn.metrics.confusion_matrix(ds2_val_Y, ds2_val_Y_predict)) # confusion matrix\n",
    "print ('\\n')\n",
    "print (sklearn.metrics.classification_report(ds2_val_Y, ds2_val_Y_predict, target_names=ds2_labels)) # precision, recall, f1-measure (macro and weighted) and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 52   0   0   0   0   0   0   0   1   2]\n [  2 121   0   1   0   0   0   0   1   0]\n [  1   0   8   0   0   1   0   0   4   1]\n [  0   0   0  14   1   0   0   0   0   0]\n [ 10   1   0   0  27   4   0   0   2   6]\n [  0   0   1   1   1  49   0   0   0   3]\n [  2   6   1   0   0   0   5   0   1   0]\n [  0   0   0   0   0   0   0  15   0   0]\n [  4   2   0   0   0   0   0   0  42   2]\n [  6   1   1   0   6   6   1   0   1 103]]\n\n\n              precision    recall  f1-score   support\n\n          pi       0.68      0.95      0.79        55\n       alpha       0.92      0.97      0.95       125\n        beta       0.73      0.53      0.62        15\n       sigma       0.88      0.93      0.90        15\n       gamma       0.77      0.54      0.64        50\n       delta       0.82      0.89      0.85        55\n      lambda       0.83      0.33      0.48        15\n       omega       1.00      1.00      1.00        15\n          mu       0.81      0.84      0.82        50\n          xi       0.88      0.82      0.85       125\n\n    accuracy                           0.84       520\n   macro avg       0.83      0.78      0.79       520\nweighted avg       0.84      0.84      0.83       520\n\n"
     ]
    }
   ],
   "source": [
    "# When ready, do testing\n",
    "ds2_test_X, ds2_test_Y = load_dataset('test_with_label_2')\n",
    "\n",
    "ds2_test_Y_predict = ds2_perceptron_model.predict(ds2_test_X)\n",
    "\n",
    "# Check test metrics\n",
    "print (sklearn.metrics.confusion_matrix(ds2_test_Y, ds2_test_Y_predict)) # confusion matrix\n",
    "print ('\\n')\n",
    "print (sklearn.metrics.classification_report(ds2_test_Y, ds2_test_Y_predict, target_names=ds2_labels)) # precision, recall, f1-measure (macro and weighted average) and accuracy\n",
    "\n",
    "# Write test results to output file PER-DS2.csv\n",
    "create_output_file(ds2_test_Y, ds2_test_Y_predict, ds2_labels, 'PER-DS2')"
   ]
  }
 ]
}