{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import useful packages\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.tree\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions and common variables\n",
    "def create_output_file(y_actual, y_predicted, labels, filename):\n",
    "    '''\n",
    "    Function for creating the ouput file as specified in the assignment instructions:\n",
    "    a) instance number and predicted label (number)\n",
    "    b) Confusion matrix\n",
    "    c) Precision, recall and f1-measure of each class\n",
    "    d) Accuracy, macro-average f1 and weighted-average f1 score of the model\n",
    "\n",
    "    y_actual: numpy array of shape (N,) containing the actual class of each test instance\n",
    "    y_predicted: numpy array of shape (N,) containing the class of each test instance predicted by the model\n",
    "    labels: 1D numpy array containing the class labels of the dataset\n",
    "    filename: name of the output (.csv) file\n",
    "    '''\n",
    "\n",
    "    # Open file\n",
    "    output = open('A1-Output/' + filename + '.csv', 'w')\n",
    "\n",
    "    # a) Write y values of test data\n",
    "    output.write('instance,prediction\\n')\n",
    "    for i in range(y_predicted.shape[0]):\n",
    "        output.write(str(i+1) + ',' + str(y_predicted[i]) + '\\n')\n",
    "\n",
    "    output.write('\\n')\n",
    "\n",
    "    # b) Plot confusion matrix\n",
    "    output.write('confusion matrix\\n')\n",
    "    confusion_matrix = sklearn.metrics.confusion_matrix(y_actual, y_predicted)\n",
    "    (m, n) = confusion_matrix.shape\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            if j < n-1:\n",
    "                output.write(str(confusion_matrix[i,j]) + ',')\n",
    "            else:\n",
    "                output.write(str(confusion_matrix[i,j]))\n",
    "        output.write('\\n')\n",
    "\n",
    "    output.write('\\n')\n",
    "\n",
    "    # c) Write precision, recall and f1-measure of each class (rounded to 2 decimals)\n",
    "    output.write('precision,recall,f1-measure\\n')\n",
    "    precision = sklearn.metrics.precision_score(y_actual, y_predicted, average=None)\n",
    "    recall = sklearn.metrics.recall_score(y_actual, y_predicted, average=None)\n",
    "    f1 = sklearn.metrics.f1_score(y_actual, y_predicted, average=None)\n",
    "\n",
    "    for i in range(labels.shape[0]):\n",
    "        #output.write(str(precision[i]) + ',' + str(recall[i]) + ',' + str(f1[i]) + '\\n')\n",
    "        output.write('{:.2f},{:.2f},{:.2f}\\n'.format(precision[i], recall[i], f1[i]))\n",
    "\n",
    "    output.write('\\n')\n",
    "\n",
    "    # Write accuracy, macro-average f1 and weighted-average f1 of the model (rounded to 2 decimals)\n",
    "    output.write('accuracy,macro-average f-1,weighted-average f1\\n') \n",
    "    accuracy = sklearn.metrics.accuracy_score(y_actual, y_predicted)\n",
    "    macro_avg_f1 = sklearn.metrics.f1_score(y_actual, y_predicted, average='macro')\n",
    "    weighted_avg_f1 = sklearn.metrics.f1_score(y_actual, y_predicted, average='weighted')\n",
    "    #output.write(str(accuracy) + ',' + str(macro_avg_f1) + ',' + str(weighted_avg_f1))\n",
    "    output.write('{:.2f},{:.2f},{:.2f}\\n'.format(accuracy, macro_avg_f1, weighted_avg_f1))\n",
    "\n",
    "    # Close output file\n",
    "    output.close()\n",
    "\n",
    "nb_pixels = 32**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load training and validation data for dataset 1\n",
    "ds1_labels = np.loadtxt('Assig1-Dataset/info_1.csv', skiprows=1, usecols=1, delimiter=',', dtype=np.str)\n",
    "\n",
    "ds1_training_X = np.loadtxt('Assig1-Dataset/train_1.csv', usecols=np.arange(nb_pixels), delimiter=',' ,dtype=np.int32)\n",
    "ds1_training_Y = np.loadtxt('Assig1-Dataset/train_1.csv', usecols=nb_pixels, delimiter=',' ,dtype=np.int32)\n",
    "\n",
    "ds1_val_X = np.loadtxt('Assig1-Dataset/val_1.csv', usecols=np.arange(nb_pixels), delimiter=',' ,dtype=np.int32)\n",
    "ds1_val_Y = np.loadtxt('Assig1-Dataset/val_1.csv', usecols=nb_pixels, delimiter=',' ,dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding best hyperparameters using grid search...\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'class_weight': None, 'criterion': 'entropy', 'max_depth': None, 'min_impurity_decrease': 0.0, 'min_samples_split': 2}\n"
     ]
    }
   ],
   "source": [
    "# Execute a grid search for the best hyperparameters to use with Best DT Model, for Data set 1\n",
    "parameters = [{'criterion': ['gini'], \n",
    "               'max_depth': [None, 10],\n",
    "               'min_samples_split': [2, 4, 6, 8, 10],\n",
    "               'min_impurity_decrease': [0.0, 0.05, 0.1, 0.2, 0.4, 0.5],\n",
    "               'class_weight': [None, 'balanced']},\n",
    "              {'criterion': ['entropy'], \n",
    "               'max_depth': [None, 10],\n",
    "               'min_samples_split': [2, 4, 6, 8, 10],\n",
    "               'min_impurity_decrease': [0.0, 0.05, 0.1, 0.2, 0.4, 0.5],\n",
    "               'class_weight': [None, 'balanced']}]\n",
    "\n",
    "print(\"Finding best hyperparameters using grid search...\" + \"\\n\")\n",
    "ds1_best_dt_model = GridSearchCV(\n",
    "    sklearn.tree.DecisionTreeClassifier(), parameters)\n",
    "ds1_best_dt_model.fit(ds1_training_X, ds1_training_Y)\n",
    "\n",
    "best_params_ds1 = ds1_best_dt_model.best_params_\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(best_params_ds1)\n",
    "\n",
    "# Train Best DT Model with dataset 1, using the hyperparameters found in the previous grid search.\n",
    "ds1_best_dt_model = sklearn.tree.DecisionTreeClassifier(criterion=best_params['criterion'],\n",
    "                                                       max_depth=best_params['max_depth'],\n",
    "                                                       min_samples_split=best_params['min_samples_split'],\n",
    "                                                       min_impurity_decrease=best_params['min_impurity_decrease'],\n",
    "                                                       class_weight=best_params['class_weight'])\n",
    "ds1_best_dt_model = ds1_best_dt_model.fit(ds1_training_X, ds1_training_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 2 2 1 0 0 0 2 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 8 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 2 0 2 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1]\n",
      " [0 0 0 0 3 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 1 0 0 2 0 0 0 0 2 0 0 0 0 0 0 0 0 2 0 0 0 0 0 1]\n",
      " [1 0 1 0 0 0 2 1 0 0 0 1 0 0 0 0 1 0 2 0 1 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 4 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [1 0 0 1 0 1 0 0 5 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0]\n",
      " [0 0 1 2 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 2]\n",
      " [1 0 0 0 0 0 0 0 0 0 7 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 7 1 0 0 0 0 0 0 0 1 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 1 0 0 0 6 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      " [0 0 0 2 0 0 0 1 0 0 1 0 1 2 0 0 0 0 0 0 0 1 1 0 1 0]\n",
      " [0 0 0 1 0 0 2 0 0 1 0 0 0 0 3 0 0 0 1 0 1 0 0 0 0 1]\n",
      " [0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 5 0 0 1 1 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 4 0 1 0 0 0 0 0 0 0]\n",
      " [0 2 0 0 0 0 0 0 0 0 0 0 0 0 1 4 0 2 1 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 3 2 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 2]\n",
      " [0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 5 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 4 0 0 0 0 0 0 0 1 2 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 5 1 0 0]\n",
      " [1 0 0 0 0 0 0 1 0 0 0 0 0 2 0 0 0 0 0 0 0 0 1 2 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 2 0 1 4 0]\n",
      " [0 1 0 0 2 1 0 0 0 2 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.60      0.90      0.72        10\n",
      "           B       0.14      0.11      0.12         9\n",
      "           C       0.62      0.80      0.70        10\n",
      "           D       0.20      0.25      0.22         8\n",
      "           E       0.38      0.38      0.38         8\n",
      "           F       0.25      0.25      0.25         8\n",
      "           G       0.25      0.20      0.22        10\n",
      "           H       0.44      0.44      0.44         9\n",
      "           I       0.62      0.50      0.56        10\n",
      "           J       0.08      0.10      0.09        10\n",
      "           K       0.50      0.70      0.58        10\n",
      "           L       0.58      0.70      0.64        10\n",
      "           M       0.55      0.60      0.57        10\n",
      "           N       0.22      0.20      0.21        10\n",
      "           O       0.50      0.30      0.37        10\n",
      "           P       0.42      0.50      0.45        10\n",
      "           Q       0.67      0.40      0.50        10\n",
      "           R       0.50      0.20      0.29        10\n",
      "           S       0.33      0.33      0.33         9\n",
      "           T       0.27      0.38      0.32         8\n",
      "           U       0.50      0.62      0.56         8\n",
      "           V       0.17      0.12      0.14         8\n",
      "           W       0.56      0.56      0.56         9\n",
      "           X       0.40      0.25      0.31         8\n",
      "           Y       0.67      0.50      0.57         8\n",
      "           Z       0.09      0.11      0.10         9\n",
      "\n",
      "    accuracy                           0.41       239\n",
      "   macro avg       0.40      0.40      0.39       239\n",
      "weighted avg       0.41      0.41      0.40       239\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use validation data to test first\n",
    "ds1_val_Y_predict = ds1_best_dt_model.predict(ds1_val_X)\n",
    "\n",
    "# Check validation metrics and modify hyper-parameters as needed in previous cell\n",
    "print (sklearn.metrics.confusion_matrix(ds1_val_Y, ds1_val_Y_predict)) # confusion matrix\n",
    "print ('\\n')\n",
    "print (sklearn.metrics.classification_report(ds1_val_Y, ds1_val_Y_predict, target_names=ds1_labels)) # precision, recall, f1-measure (macro and weighted) and accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 2 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 2 0 1 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.60      0.75      0.67         4\n",
      "           B       0.00      0.00      0.00         2\n",
      "           C       0.75      0.75      0.75         4\n",
      "           D       0.50      0.33      0.40         3\n",
      "           E       0.33      0.50      0.40         2\n",
      "           F       0.33      0.50      0.40         2\n",
      "           G       0.67      0.50      0.57         4\n",
      "           H       1.00      0.33      0.50         3\n",
      "           I       0.50      0.33      0.40         3\n",
      "           J       0.67      0.50      0.57         4\n",
      "           K       0.00      0.00      0.00         3\n",
      "           L       1.00      0.75      0.86         4\n",
      "           M       0.00      0.00      0.00         3\n",
      "           N       0.50      0.75      0.60         4\n",
      "           O       0.50      0.67      0.57         3\n",
      "           P       0.40      0.67      0.50         3\n",
      "           Q       1.00      0.33      0.50         3\n",
      "           R       0.67      0.50      0.57         4\n",
      "           S       1.00      0.67      0.80         3\n",
      "           T       0.33      0.50      0.40         2\n",
      "           U       0.50      1.00      0.67         3\n",
      "           V       0.50      0.33      0.40         3\n",
      "           W       0.50      0.67      0.57         3\n",
      "           X       0.33      0.50      0.40         2\n",
      "           Y       0.33      0.67      0.44         3\n",
      "           Z       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.50        80\n",
      "   macro avg       0.50      0.48      0.46        80\n",
      "weighted avg       0.53      0.50      0.49        80\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joel\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Joel\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# When ready, do testing\n",
    "ds1_test_X = np.loadtxt('Assig1-Dataset/test_no_label_1.csv', delimiter=',' ,dtype=np.int32)\n",
    "ds1_test_Y = np.loadtxt('Assig1-Dataset/test_with_label_1.csv', usecols=nb_pixels, delimiter=',' ,dtype=np.int32)\n",
    "\n",
    "ds1_test_Y_predict = ds1_best_dt_model.predict(ds1_test_X)\n",
    "\n",
    "# Check test metrics\n",
    "print (sklearn.metrics.confusion_matrix(ds1_test_Y, ds1_test_Y_predict)) # confusion matrix\n",
    "print ('\\n')\n",
    "print (sklearn.metrics.classification_report(ds1_test_Y, ds1_test_Y_predict, target_names=ds1_labels)) # precision, recall, f1-measure (macro and weighted average) and accuracy\n",
    "\n",
    "# Write test results to output file Best-DT-DS1.csv\n",
    "create_output_file(ds1_test_Y, ds1_test_Y_predict, ds1_labels, 'Best-DT-DS1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and validation data for dataset 2\n",
    "ds2_labels = np.loadtxt('Assig1-Dataset/info_2.csv', skiprows=1, usecols=1, delimiter=',', dtype=np.str)\n",
    "\n",
    "ds2_training_X = np.loadtxt('Assig1-Dataset/train_2.csv', usecols=np.arange(nb_pixels), delimiter=',' ,dtype=np.int32)\n",
    "ds2_training_Y = np.loadtxt('Assig1-Dataset/train_2.csv', usecols=nb_pixels, delimiter=',' ,dtype=np.int32)\n",
    "\n",
    "ds2_val_X = np.loadtxt('Assig1-Dataset/val_2.csv', usecols=np.arange(nb_pixels), delimiter=',' ,dtype=np.int32)\n",
    "ds2_val_Y = np.loadtxt('Assig1-Dataset/val_2.csv', usecols=nb_pixels, delimiter=',' ,dtype=np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding best hyperparameters using grid search...\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': None, 'min_impurity_decrease': 0.0, 'min_samples_split': 2}\n"
     ]
    }
   ],
   "source": [
    "# Execute a grid search for the best hyperparameters to use with Best DT Model, for Data set 2\n",
    "parameters = [{'criterion': ['gini'], \n",
    "               'max_depth': [None, 10],\n",
    "               'min_samples_split': [2, 4, 6, 8, 10],\n",
    "               'min_impurity_decrease': [0.0, 0.05, 0.1, 0.2, 0.4, 0.5],\n",
    "               'class_weight': [None, 'balanced']},\n",
    "              {'criterion': ['entropy'], \n",
    "               'max_depth': [None, 10],\n",
    "               'min_samples_split': [2, 4, 6, 8, 10],\n",
    "               'min_impurity_decrease': [0.0, 0.05, 0.1, 0.2, 0.4, 0.5],\n",
    "               'class_weight': [None, 'balanced']}]\n",
    "\n",
    "print(\"Finding best hyperparameters using grid search...\" + \"\\n\")\n",
    "ds2_best_dt_model = GridSearchCV(\n",
    "    sklearn.tree.DecisionTreeClassifier(), parameters)\n",
    "ds2_best_dt_model.fit(ds2_training_X, ds2_training_Y)\n",
    "\n",
    "best_params_ds2 = ds2_best_dt_model.best_params_\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(best_params_ds2)\n",
    "\n",
    "# Train Best DT Model with dataset 1, using the hyperparameters found in the previous grid search.\n",
    "ds2_best_dt_model = sklearn.tree.DecisionTreeClassifier(criterion=best_params['criterion'],\n",
    "                                                       max_depth=best_params['max_depth'],\n",
    "                                                       min_samples_split=best_params['min_samples_split'],\n",
    "                                                       min_impurity_decrease=best_params['min_impurity_decrease'],\n",
    "                                                       class_weight=best_params['class_weight'])\n",
    "ds2_best_dt_model = ds2_best_dt_model.fit(ds2_training_X, ds2_training_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[121   9   1   1  19   3   3   1   2   5]\n",
      " [  7 338   2  21   2   3   1   0   0   1]\n",
      " [  4   2  26   0   2   1   0   0   5   5]\n",
      " [  3  15   2  20   2   0   0   2   0   1]\n",
      " [ 13   7   1   1  81  12   2   1   4  28]\n",
      " [  1   4   3   3   4 125   0   0   1  24]\n",
      " [  1   1   0   0   6   0  31   0   1   5]\n",
      " [  2   2   0   0   0   3   1  37   0   0]\n",
      " [  3   7   1   2   5   1   2   0 120   9]\n",
      " [  7   3   8   2  29  39   3   0   7 277]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          pi       0.75      0.73      0.74       165\n",
      "       alpha       0.87      0.90      0.89       375\n",
      "        beta       0.59      0.58      0.58        45\n",
      "       sigma       0.40      0.44      0.42        45\n",
      "       gamma       0.54      0.54      0.54       150\n",
      "       delta       0.67      0.76      0.71       165\n",
      "      lambda       0.72      0.69      0.70        45\n",
      "       omega       0.90      0.82      0.86        45\n",
      "          mu       0.86      0.80      0.83       150\n",
      "          xi       0.78      0.74      0.76       375\n",
      "\n",
      "    accuracy                           0.75      1560\n",
      "   macro avg       0.71      0.70      0.70      1560\n",
      "weighted avg       0.76      0.75      0.75      1560\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use validation data to test first\n",
    "ds2_val_Y_predict = ds2_best_dt_model.predict(ds2_val_X)\n",
    "\n",
    "# Check validation metrics and adjust hyper-parameters as needed in previous cell\n",
    "print (sklearn.metrics.confusion_matrix(ds2_val_Y, ds2_val_Y_predict)) # confusion matrix\n",
    "print ('\\n')\n",
    "print (sklearn.metrics.classification_report(ds2_val_Y, ds2_val_Y_predict, target_names=ds2_labels)) # precision, recall, f1-measure (macro and weighted) and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 45   0   2   0   6   0   0   0   0   2]\n",
      " [  0 114   0   5   2   2   0   1   1   0]\n",
      " [  3   0   6   0   1   1   0   0   2   2]\n",
      " [  0   2   0   9   1   2   0   1   0   0]\n",
      " [  3   1   0   1  24   2   1   1   2  15]\n",
      " [  0   0   0   1   5  41   1   0   0   7]\n",
      " [  0   0   2   0   2   0   7   0   2   2]\n",
      " [  0   1   0   0   0   0   0  13   1   0]\n",
      " [  1   1   0   0   2   0   0   0  43   3]\n",
      " [  1   2   2   0   9  12   0   0   1  98]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          pi       0.85      0.82      0.83        55\n",
      "       alpha       0.94      0.91      0.93       125\n",
      "        beta       0.50      0.40      0.44        15\n",
      "       sigma       0.56      0.60      0.58        15\n",
      "       gamma       0.46      0.48      0.47        50\n",
      "       delta       0.68      0.75      0.71        55\n",
      "      lambda       0.78      0.47      0.58        15\n",
      "       omega       0.81      0.87      0.84        15\n",
      "          mu       0.83      0.86      0.84        50\n",
      "          xi       0.76      0.78      0.77       125\n",
      "\n",
      "    accuracy                           0.77       520\n",
      "   macro avg       0.72      0.69      0.70       520\n",
      "weighted avg       0.77      0.77      0.77       520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# When ready, do testing\n",
    "ds2_test_X = np.loadtxt('Assig1-Dataset/test_no_label_2.csv', delimiter=',' ,dtype=np.int32)\n",
    "ds2_test_Y = np.loadtxt('Assig1-Dataset/test_with_label_2.csv', usecols=nb_pixels, delimiter=',' ,dtype=np.int32)\n",
    "\n",
    "ds2_test_Y_predict = ds2_best_dt_model.predict(ds2_test_X)\n",
    "\n",
    "# Check test metrics\n",
    "print (sklearn.metrics.confusion_matrix(ds2_test_Y, ds2_test_Y_predict)) # confusion matrix\n",
    "print ('\\n')\n",
    "print (sklearn.metrics.classification_report(ds2_test_Y, ds2_test_Y_predict, target_names=ds2_labels)) # precision, recall, f1-measure (macro and weighted average) and accuracy\n",
    "\n",
    "# Write test results to output file Best-DT-DS2.csv\n",
    "create_output_file(ds2_test_Y, ds2_test_Y_predict, ds2_labels, 'Best-DT-DS2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
