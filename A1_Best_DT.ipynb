{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import useful packages\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.tree\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions and common variables\n",
    "def create_output_file(y_actual, y_predicted, labels, filename):\n",
    "    '''\n",
    "    Function for creating the ouput file as specified in the assignment instructions:\n",
    "    a) instance number and predicted label (number)\n",
    "    b) Confusion matrix\n",
    "    c) Precision, recall and f1-measure of each class\n",
    "    d) Accuracy, macro-average f1 and weighted-average f1 score of the model\n",
    "\n",
    "    y_actual: numpy array of shape (N,) containing the actual class of each test instance\n",
    "    y_predicted: numpy array of shape (N,) containing the class of each test instance predicted by the model\n",
    "    labels: 1D numpy array containing the class labels of the dataset\n",
    "    filename: name of the output (.csv) file\n",
    "    '''\n",
    "\n",
    "    # Open file\n",
    "    output = open('A1-Output/' + filename + '.csv', 'w')\n",
    "\n",
    "    # a) Write y values of test data\n",
    "    output.write('instance,prediction\\n')\n",
    "    for i in range(y_predicted.shape[0]):\n",
    "        output.write(str(i+1) + ',' + str(y_predicted[i]) + '\\n')\n",
    "\n",
    "    output.write('\\n')\n",
    "\n",
    "    # b) Plot confusion matrix\n",
    "    output.write('confusion matrix\\n')\n",
    "    confusion_matrix = sklearn.metrics.confusion_matrix(y_actual, y_predicted)\n",
    "    (m, n) = confusion_matrix.shape\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            if j < n-1:\n",
    "                output.write(str(confusion_matrix[i,j]) + ',')\n",
    "            else:\n",
    "                output.write(str(confusion_matrix[i,j]))\n",
    "        output.write('\\n')\n",
    "\n",
    "    output.write('\\n')\n",
    "\n",
    "    # c) Write precision, recall and f1-measure of each class (rounded to 2 decimals)\n",
    "    output.write('precision,recall,f1-measure\\n')\n",
    "    precision = sklearn.metrics.precision_score(y_actual, y_predicted, average=None)\n",
    "    recall = sklearn.metrics.recall_score(y_actual, y_predicted, average=None)\n",
    "    f1 = sklearn.metrics.f1_score(y_actual, y_predicted, average=None)\n",
    "\n",
    "    for i in range(labels.shape[0]):\n",
    "        #output.write(str(precision[i]) + ',' + str(recall[i]) + ',' + str(f1[i]) + '\\n')\n",
    "        output.write('{:.2f},{:.2f},{:.2f}\\n'.format(precision[i], recall[i], f1[i]))\n",
    "\n",
    "    output.write('\\n')\n",
    "\n",
    "    # Write accuracy, macro-average f1 and weighted-average f1 of the model (rounded to 2 decimals)\n",
    "    output.write('accuracy,macro-average f-1,weighted-average f1\\n') \n",
    "    accuracy = sklearn.metrics.accuracy_score(y_actual, y_predicted)\n",
    "    macro_avg_f1 = sklearn.metrics.f1_score(y_actual, y_predicted, average='macro')\n",
    "    weighted_avg_f1 = sklearn.metrics.f1_score(y_actual, y_predicted, average='weighted')\n",
    "    #output.write(str(accuracy) + ',' + str(macro_avg_f1) + ',' + str(weighted_avg_f1))\n",
    "    output.write('{:.2f},{:.2f},{:.2f}\\n'.format(accuracy, macro_avg_f1, weighted_avg_f1))\n",
    "\n",
    "    # Close output file\n",
    "    output.close()\n",
    "\n",
    "def load_dataset(filename, nb_pixels=32**2):\n",
    "    '''\n",
    "    Function for loading the X and Y data of the passed csv file\n",
    "\n",
    "    filename: name of the file containing the dataset (ex: train_1)\n",
    "\n",
    "    Return: \n",
    "    X: 2D numpy array containing the value of the features of each instance\n",
    "    Y: 1D numpy array containing the true class of each instance \n",
    "    '''\n",
    "    data = np.loadtxt('Assig1-Dataset/' + filename + '.csv', delimiter=',', dtype=np.int32)\n",
    "    return data[:, :nb_pixels], data[:, nb_pixels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load training and validation data for dataset 1\n",
    "ds1_labels = np.loadtxt('Assig1-Dataset/info_1.csv', skiprows=1, usecols=1, delimiter=',', dtype=np.str)\n",
    "\n",
    "ds1_training_X, ds1_training_Y = load_dataset('train_1')\n",
    "ds1_val_X, ds1_val_Y = load_dataset('val_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding best hyperparameters using grid search...\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'class_weight': None, 'criterion': 'entropy', 'max_depth': None, 'min_impurity_decrease': 0.0, 'min_samples_split': 3}\n"
     ]
    }
   ],
   "source": [
    "# Execute a grid search for the best hyperparameters to use with Best DT Model, for Data set 1\n",
    "parameters = [{'criterion': ['gini'], \n",
    "               'max_depth': [None, 10],\n",
    "               'min_samples_split': [2, 3, 4, 5, 6],\n",
    "               'min_impurity_decrease': [0.0, 0.02, 0.05, 0.1, 0.15, 0.2],\n",
    "               'class_weight': [None, 'balanced']},\n",
    "              {'criterion': ['entropy'], \n",
    "               'max_depth': [None, 10],\n",
    "               'min_samples_split': [2, 3, 4, 5, 6],\n",
    "               'min_impurity_decrease': [0.0, 0.02, 0.05, 0.1, 0.15, 0.2],\n",
    "               'class_weight': [None, 'balanced']}]\n",
    "\n",
    "print(\"Finding best hyperparameters using grid search...\" + \"\\n\")\n",
    "ds1_best_dt_model = GridSearchCV(\n",
    "    sklearn.tree.DecisionTreeClassifier(), parameters)\n",
    "ds1_best_dt_model.fit(ds1_training_X, ds1_training_Y)\n",
    "\n",
    "best_params_ds1 = ds1_best_dt_model.best_params_\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(best_params_ds1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Best DT Model with dataset 1, using the hyperparameters found in the previous grid search.\n",
    "ds1_best_dt_model = sklearn.tree.DecisionTreeClassifier(criterion=best_params_ds1['criterion'],\n",
    "                                                       max_depth=best_params_ds1['max_depth'],\n",
    "                                                       min_samples_split=best_params_ds1['min_samples_split'],\n",
    "                                                       min_impurity_decrease=best_params_ds1['min_impurity_decrease'],\n",
    "                                                       class_weight=best_params_ds1['class_weight'])\n",
    "ds1_best_dt_model = ds1_best_dt_model.fit(ds1_training_X, ds1_training_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0]\n",
      " [0 1 0 3 1 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1]\n",
      " [0 0 8 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 3 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0]\n",
      " [0 1 0 0 3 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 1 3 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [1 0 1 0 0 0 3 0 0 0 0 2 0 0 0 0 0 0 2 0 0 0 0 1 0 0]\n",
      " [1 0 0 0 0 0 0 4 1 0 1 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0]\n",
      " [1 0 0 1 0 0 0 0 5 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0]\n",
      " [0 1 0 1 2 0 0 0 0 1 0 1 0 0 0 0 0 2 0 1 0 0 0 0 0 1]\n",
      " [2 0 0 0 0 0 0 0 0 0 6 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 8 1 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 1 0 1 0 5 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      " [0 0 0 1 0 0 0 1 0 0 1 0 1 3 0 0 0 0 0 0 0 1 1 0 1 0]\n",
      " [0 0 0 1 1 0 1 0 0 0 0 0 0 0 5 0 0 0 1 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 2 2 0 0 1 0 0 0 0 0 0 4 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 1 0 0 0 0 1 0 0 0 4 0 2 0 1 0 0 0 0 0 0 0]\n",
      " [0 2 0 0 0 0 0 0 0 0 0 0 0 0 1 4 0 2 0 0 0 0 0 0 0 1]\n",
      " [0 0 1 0 2 0 3 0 0 0 0 0 0 0 0 0 1 0 2 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 5 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 6 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 2 0 0 3 0 0 0 0 0 0 0 1 2 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 5 1 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 4 0 1]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 5 0]\n",
      " [0 2 0 0 2 0 0 0 0 2 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.47      0.70      0.56        10\n",
      "           B       0.11      0.11      0.11         9\n",
      "           C       0.73      0.80      0.76        10\n",
      "           D       0.30      0.38      0.33         8\n",
      "           E       0.21      0.38      0.27         8\n",
      "           F       0.38      0.38      0.38         8\n",
      "           G       0.30      0.30      0.30        10\n",
      "           H       0.67      0.44      0.53         9\n",
      "           I       0.62      0.50      0.56        10\n",
      "           J       0.12      0.10      0.11        10\n",
      "           K       0.46      0.60      0.52        10\n",
      "           L       0.62      0.80      0.70        10\n",
      "           M       0.50      0.50      0.50        10\n",
      "           N       0.33      0.30      0.32        10\n",
      "           O       0.42      0.50      0.45        10\n",
      "           P       0.50      0.40      0.44        10\n",
      "           Q       0.50      0.20      0.29        10\n",
      "           R       0.29      0.20      0.24        10\n",
      "           S       0.25      0.22      0.24         9\n",
      "           T       0.62      0.62      0.62         8\n",
      "           U       0.55      0.75      0.63         8\n",
      "           V       0.14      0.12      0.13         8\n",
      "           W       0.62      0.56      0.59         9\n",
      "           X       0.50      0.50      0.50         8\n",
      "           Y       0.62      0.62      0.62         8\n",
      "           Z       0.17      0.11      0.13         9\n",
      "\n",
      "    accuracy                           0.43       239\n",
      "   macro avg       0.42      0.43      0.42       239\n",
      "weighted avg       0.42      0.43      0.42       239\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use validation data to test first\n",
    "ds1_val_Y_predict = ds1_best_dt_model.predict(ds1_val_X)\n",
    "\n",
    "# Check validation metrics and modify hyper-parameters as needed in previous cell\n",
    "print (sklearn.metrics.confusion_matrix(ds1_val_Y, ds1_val_Y_predict)) # confusion matrix\n",
    "print ('\\n')\n",
    "print (sklearn.metrics.classification_report(ds1_val_Y, ds1_val_Y_predict, target_names=ds1_labels)) # precision, recall, f1-measure (macro and weighted) and accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 3 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 2 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 2 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.60      0.75      0.67         4\n",
      "           B       0.00      0.00      0.00         2\n",
      "           C       0.75      0.75      0.75         4\n",
      "           D       0.50      0.33      0.40         3\n",
      "           E       0.33      0.50      0.40         2\n",
      "           F       0.50      0.50      0.50         2\n",
      "           G       0.40      0.50      0.44         4\n",
      "           H       1.00      0.33      0.50         3\n",
      "           I       1.00      0.33      0.50         3\n",
      "           J       0.50      0.75      0.60         4\n",
      "           K       0.00      0.00      0.00         3\n",
      "           L       0.75      0.75      0.75         4\n",
      "           M       0.00      0.00      0.00         3\n",
      "           N       0.40      0.50      0.44         4\n",
      "           O       0.50      0.67      0.57         3\n",
      "           P       0.33      0.33      0.33         3\n",
      "           Q       1.00      0.33      0.50         3\n",
      "           R       0.67      0.50      0.57         4\n",
      "           S       1.00      0.67      0.80         3\n",
      "           T       0.50      0.50      0.50         2\n",
      "           U       0.60      1.00      0.75         3\n",
      "           V       0.50      0.67      0.57         3\n",
      "           W       0.25      0.33      0.29         3\n",
      "           X       0.33      0.50      0.40         2\n",
      "           Y       0.50      0.67      0.57         3\n",
      "           Z       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.49        80\n",
      "   macro avg       0.50      0.47      0.45        80\n",
      "weighted avg       0.51      0.49      0.47        80\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joel\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Joel\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# When ready, do testing\n",
    "ds1_test_X, ds1_test_Y = load_dataset('test_with_label_1')\n",
    "\n",
    "ds1_test_Y_predict = ds1_best_dt_model.predict(ds1_test_X)\n",
    "\n",
    "# Check test metrics\n",
    "print (sklearn.metrics.confusion_matrix(ds1_test_Y, ds1_test_Y_predict)) # confusion matrix\n",
    "print ('\\n')\n",
    "print (sklearn.metrics.classification_report(ds1_test_Y, ds1_test_Y_predict, target_names=ds1_labels)) # precision, recall, f1-measure (macro and weighted average) and accuracy\n",
    "\n",
    "# Write test results to output file Best-DT-DS1.csv\n",
    "create_output_file(ds1_test_Y, ds1_test_Y_predict, ds1_labels, 'Best-DT-DS1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and validation data for dataset 2\n",
    "ds2_labels = np.loadtxt('Assig1-Dataset/info_2.csv', skiprows=1, usecols=1, delimiter=',', dtype=np.str)\n",
    "\n",
    "ds2_training_X, ds2_training_Y = load_dataset('train_2')\n",
    "ds2_val_X, ds2_val_Y = load_dataset('val_2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding best hyperparameters using grid search...\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': None, 'min_impurity_decrease': 0.0, 'min_samples_split': 2}\n"
     ]
    }
   ],
   "source": [
    "# Execute a grid search for the best hyperparameters to use with Best DT Model, for Data set 2\n",
    "parameters = [{'criterion': ['gini'], \n",
    "               'max_depth': [None, 10],\n",
    "               'min_samples_split': [2, 3, 4, 5, 6],\n",
    "               'min_impurity_decrease': [0.0, 0.02, 0.05, 0.1, 0.15, 0.2],\n",
    "               'class_weight': [None, 'balanced']},\n",
    "              {'criterion': ['entropy'], \n",
    "               'max_depth': [None, 10],\n",
    "               'min_samples_split': [2, 3, 4, 5, 6],\n",
    "               'min_impurity_decrease': [0.0, 0.02, 0.05, 0.1, 0.15, 0.2],\n",
    "               'class_weight': [None, 'balanced']}]\n",
    "\n",
    "print(\"Finding best hyperparameters using grid search...\" + \"\\n\")\n",
    "ds2_best_dt_model = GridSearchCV(\n",
    "    sklearn.tree.DecisionTreeClassifier(), parameters)\n",
    "ds2_best_dt_model.fit(ds2_training_X, ds2_training_Y)\n",
    "\n",
    "best_params_ds2 = ds2_best_dt_model.best_params_\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(best_params_ds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Best DT Model with dataset 1, using the hyperparameters found in the previous grid search.\n",
    "ds2_best_dt_model = sklearn.tree.DecisionTreeClassifier(criterion=best_params_ds2['criterion'],\n",
    "                                                       max_depth=best_params_ds2['max_depth'],\n",
    "                                                       min_samples_split=best_params_ds2['min_samples_split'],\n",
    "                                                       min_impurity_decrease=best_params_ds2['min_impurity_decrease'],\n",
    "                                                       class_weight=best_params_ds2['class_weight'])\n",
    "ds2_best_dt_model = ds2_best_dt_model.fit(ds2_training_X, ds2_training_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[125   7   0   2   9   3   3   0   8   8]\n",
      " [  7 343   0   7   2   1   3   3   4   5]\n",
      " [  4   0  31   1   1   2   1   0   2   3]\n",
      " [  1   9   0  30   0   0   0   3   0   2]\n",
      " [ 11   0   2   3  90  11   1   1   4  27]\n",
      " [  2   3   3   1  10 106   0   0   1  39]\n",
      " [  3   0   1   0   1   0  29   0   3   8]\n",
      " [  0   4   0   0   1   2   0  38   0   0]\n",
      " [  6   9   6   0   4   1   1   0 117   6]\n",
      " [  8   8   5   3  27  22   4   2   7 289]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          pi       0.75      0.76      0.75       165\n",
      "       alpha       0.90      0.91      0.91       375\n",
      "        beta       0.65      0.69      0.67        45\n",
      "       sigma       0.64      0.67      0.65        45\n",
      "       gamma       0.62      0.60      0.61       150\n",
      "       delta       0.72      0.64      0.68       165\n",
      "      lambda       0.69      0.64      0.67        45\n",
      "       omega       0.81      0.84      0.83        45\n",
      "          mu       0.80      0.78      0.79       150\n",
      "          xi       0.75      0.77      0.76       375\n",
      "\n",
      "    accuracy                           0.77      1560\n",
      "   macro avg       0.73      0.73      0.73      1560\n",
      "weighted avg       0.77      0.77      0.77      1560\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use validation data to test first\n",
    "ds2_val_Y_predict = ds2_best_dt_model.predict(ds2_val_X)\n",
    "\n",
    "# Check validation metrics and adjust hyper-parameters as needed in previous cell\n",
    "print (sklearn.metrics.confusion_matrix(ds2_val_Y, ds2_val_Y_predict)) # confusion matrix\n",
    "print ('\\n')\n",
    "print (sklearn.metrics.classification_report(ds2_val_Y, ds2_val_Y_predict, target_names=ds2_labels)) # precision, recall, f1-measure (macro and weighted) and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 41   2   3   0   0   0   3   0   4   2]\n",
      " [  0 115   0   1   0   1   2   1   2   3]\n",
      " [  1   0   4   0   0   1   0   0   3   6]\n",
      " [  1   2   0  11   1   0   0   0   0   0]\n",
      " [  4   0   0   1  31   3   1   0   1   9]\n",
      " [  1   1   1   0   1  43   0   0   0   8]\n",
      " [  2   0   0   0   2   0   7   0   2   2]\n",
      " [  0   0   0   0   0   0   0  15   0   0]\n",
      " [  0   0   0   0   0   0   0   0  43   7]\n",
      " [  1   5   4   0  10   8   3   0   2  92]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          pi       0.80      0.75      0.77        55\n",
      "       alpha       0.92      0.92      0.92       125\n",
      "        beta       0.33      0.27      0.30        15\n",
      "       sigma       0.85      0.73      0.79        15\n",
      "       gamma       0.69      0.62      0.65        50\n",
      "       delta       0.77      0.78      0.77        55\n",
      "      lambda       0.44      0.47      0.45        15\n",
      "       omega       0.94      1.00      0.97        15\n",
      "          mu       0.75      0.86      0.80        50\n",
      "          xi       0.71      0.74      0.72       125\n",
      "\n",
      "    accuracy                           0.77       520\n",
      "   macro avg       0.72      0.71      0.72       520\n",
      "weighted avg       0.77      0.77      0.77       520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# When ready, do testing\n",
    "ds2_test_X, ds2_test_Y = load_dataset('test_with_label_2')\n",
    "\n",
    "ds2_test_Y_predict = ds2_best_dt_model.predict(ds2_test_X)\n",
    "\n",
    "# Check test metrics\n",
    "print (sklearn.metrics.confusion_matrix(ds2_test_Y, ds2_test_Y_predict)) # confusion matrix\n",
    "print ('\\n')\n",
    "print (sklearn.metrics.classification_report(ds2_test_Y, ds2_test_Y_predict, target_names=ds2_labels)) # precision, recall, f1-measure (macro and weighted average) and accuracy\n",
    "\n",
    "# Write test results to output file Best-DT-DS2.csv\n",
    "create_output_file(ds2_test_Y, ds2_test_Y_predict, ds2_labels, 'Best-DT-DS2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
